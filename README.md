# Lindex - AI Chat Application with Ollama

An AI chat application with Ollama integration, featuring strict knowledge guardrails to prevent hallucination. Built with TypeScript, Express, and React.

## Features

- **ğŸ¤– Ollama Integration**: Local LLM integration with configurable models
- **ğŸ›¡ï¸ Knowledge Guardrails**: Strict system prompts and response filtering to prevent hallucination
- **ğŸ’¬ Real-time Chat**: Beautiful, responsive chat interface built with React
- **ğŸ”’ Type Safety**: Full TypeScript coverage with strict mode enabled
- **ğŸ“ Structured Logging**: JSON-formatted logs with correlation IDs for request tracing
- **âš¡ Modern Stack**: Express.js backend, React frontend, Vite for fast dev experience
- **ğŸ¨ Beautiful UI**: Gradient design, smooth animations, responsive layout

## Quick Start

### Prerequisites

- **Node.js** v18 or higher
- **Ollama** installed and running locally
- A downloaded Ollama model (e.g., llama2)

### Installation

```bash
# 1. Install dependencies
npm install

# 2. Install and start Ollama (if not already installed)
# macOS:
brew install ollama
ollama serve

# Windows: Download from https://ollama.ai/download/windows

# Linux:
curl https://ollama.ai/install.sh | sh

# 3. Download a model
ollama pull llama2

# 4. Environment variables are already configured in .env
# Modify if needed for custom setup

# 5. Start the application (both frontend and backend)
npm run dev
```

The application will be available at:

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:3000

### Detailed Setup

For detailed setup instructions, troubleshooting, and configuration options, see [SETUP.md](SETUP.md).

## How It Works

### Knowledge Guardrails

The application enforces strict guardrails to prevent AI hallucination:

1. **System Prompt**: Instructs the model to only answer using known information
2. **Response Filtering**: Detects uncertainty phrases like "I don't know" or "I'm not sure"
3. **Fallback Response**: Returns `"Sorry I do not know this information"` when:
   - No relevant context is available
   - The model expresses uncertainty
   - An error occurs during processing

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React     â”‚ â”€â”€HTTPâ”€â†’â”‚  Express    â”‚ â”€â”€HTTPâ”€â†’â”‚   Ollama    â”‚
â”‚  Frontend   â”‚ â†â”€JSONâ”€â”€â”‚   API       â”‚ â†â”€JSONâ”€â”€â”‚   Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. User sends message via chat UI
2. Frontend calls `/api/chat` endpoint
3. Backend validates request with Zod schemas
4. System prompt is prepended to conversation
5. Request sent to Ollama with timeout
6. Response filtered for uncertainty
7. Fallback returned if no confident answer
8. Frontend displays response

## Project Structure

```
lindex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server/                 # Backend (Express + TypeScript)
â”‚   â”‚   â”œâ”€â”€ index.ts            # Server entry point
â”‚   â”‚   â”œâ”€â”€ config.ts           # Environment configuration
â”‚   â”‚   â”œâ”€â”€ logger.ts           # Structured logging
â”‚   â”‚   â”œâ”€â”€ middleware.ts       # Express middleware
â”‚   â”‚   â”œâ”€â”€ ollama-client.ts    # Ollama API client with guardrails
â”‚   â”‚   â”œâ”€â”€ types.ts            # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ chat.ts         # Chat API endpoints
â”‚   â””â”€â”€ client/                 # Frontend (React + TypeScript)
â”‚       â”œâ”€â”€ main.tsx            # React entry point
â”‚       â”œâ”€â”€ App.tsx             # Root component
â”‚       â”œâ”€â”€ types.ts            # TypeScript types
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â””â”€â”€ chat.ts         # API client
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ ChatBox.tsx     # Chat UI component
â”‚           â””â”€â”€ ChatBox.css     # Component styles
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ package.json                # Dependencies and scripts
â”œâ”€â”€ tsconfig.json               # Base TypeScript config
â”œâ”€â”€ tsconfig.server.json        # Server TypeScript config
â”œâ”€â”€ tsconfig.client.json        # Client TypeScript config
â”œâ”€â”€ vite.config.ts              # Vite configuration
â”œâ”€â”€ index.html                  # HTML template
â”œâ”€â”€ SETUP.md                    # Detailed setup guide
â””â”€â”€ README.md                   # This file
```

## Available Scripts

```bash
# Development
npm run dev              # Run both frontend and backend
npm run dev:server       # Run backend only
npm run dev:client       # Run frontend only

# Build
npm run build            # Build both frontend and backend
npm run build:server     # Build backend only
npm run build:client     # Build frontend only

# Production
npm start                # Start production server

# Code Quality
npm run lint             # Run ESLint
npm run type-check       # Run TypeScript type checking
```

## API Endpoints

### `POST /api/chat`

Send a chat message and receive a response.

**Request:**

```json
{
  "messages": [
    { "role": "user", "content": "Hello" },
    { "role": "assistant", "content": "Hi!" }
  ],
  "userMessage": "What is Python?"
}
```

**Response:**

```json
{
  "reply": "Python is a high-level programming language...",
  "conversationId": "req-abc-123"
}
```

### `GET /api/health`

Check API and Ollama service health.

**Response:**

```json
{
  "status": "healthy",
  "ollama": "connected"
}
```

### `GET /health/live`

Liveness probe for container orchestration.

**Response:**

```json
{
  "status": "ok"
}
```

## Configuration

All configuration is done via environment variables in `.env`:

| Variable          | Default                  | Description          |
| ----------------- | ------------------------ | -------------------- |
| `PORT`            | `3000`                   | Backend server port  |
| `NODE_ENV`        | `development`            | Environment mode     |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama API URL       |
| `OLLAMA_MODEL`    | `llama2`                 | Ollama model to use  |
| `OLLAMA_TIMEOUT`  | `30000`                  | Request timeout (ms) |
| `CORS_ORIGIN`     | `http://localhost:5173`  | Allowed CORS origin  |

## Customization

### Change the AI Model

```bash
# Pull a different model
ollama pull mistral

# Update .env
OLLAMA_MODEL=mistral

# Restart the server
```

### Modify System Prompt

Edit the `SYSTEM_PROMPT` constant in `src/server/ollama-client.ts` to customize AI behavior.

### Adjust Response Filtering

Modify the `isUncertainResponse()` method in `src/server/ollama-client.ts` to change uncertainty detection.

## Troubleshooting

### Ollama Connection Failed

**Error:** "Chat service is temporarily unavailable"

**Solutions:**

1. Verify Ollama is running: `curl http://localhost:11434/api/tags`
2. Check model is downloaded: `ollama list`
3. Restart Ollama service
4. Verify `OLLAMA_BASE_URL` in `.env`

### Port Already in Use

Change `PORT` in `.env` to a different port (e.g., 3001).

### Slow Responses

1. Use a smaller model: `ollama pull llama2:7b`
2. Update `OLLAMA_MODEL=llama2:7b` in `.env`
3. Increase `OLLAMA_TIMEOUT` if needed

See [SETUP.md](SETUP.md) for more troubleshooting tips.

## Technology Stack

- **Backend**: Express.js, TypeScript, Zod
- **Frontend**: React, TypeScript, Vite
- **AI**: Ollama (local LLM runtime)
- **Validation**: Zod schemas
- **Logging**: Structured JSON logs with correlation IDs
- **Styling**: Modern CSS with gradients and animations

## Security Features

Following the `.cursorrules` security requirements:

- âœ… Input validation with Zod schemas
- âœ… Request size limits (1MB max)
- âœ… Timeout protection (30s default)
- âœ… CORS configuration
- âœ… No sensitive data in logs
- âœ… Error messages sanitized for users
- âœ… Correlation IDs for request tracing
- âœ… TypeScript strict mode enabled
- âœ… Graceful error handling

## Next Steps

- [ ] Add user authentication
- [ ] Implement conversation persistence
- [ ] Add document upload for RAG
- [ ] Deploy to production
- [ ] Add streaming responses
- [ ] Implement rate limiting

## Resources

- [Ollama Documentation](https://ollama.ai/docs)
- [TypeScript Documentation](https://www.typescriptlang.org/docs/)
- [React Documentation](https://react.dev/)
- [Express Documentation](https://expressjs.com/)
- [Vite Documentation](https://vitejs.dev/)

## License

MIT License - Free to use for any purpose.
